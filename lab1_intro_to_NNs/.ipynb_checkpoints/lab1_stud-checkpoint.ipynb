{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Laboratory of Machine Learning per il Calcolo Sceintifico\n",
    "In this notebook, we want to create the network from the paper \"Error bounds for approximations with deep ReLU networks\" by D. Yarotsky (Neural Netowrks, 2017, https://doi.org/10.1016/j.neunet.2017.07.002)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step zero: libraries\n",
    "\n",
    "Install  Python (https://www.python.org/)\n",
    "\n",
    "And the following libraries:\n",
    "\n",
    "-PyTorch (https://pytorch.org/)\n",
    "\n",
    "-Numpy (https://numpy.org/)\n",
    "\n",
    "-Matplotlib (https://matplotlib.org/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device) # default tensor device\n",
    "print(\"I'm using: \", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step: Define $\\Phi^\\wedge$ using torch Tensor \n",
    "Define the weights and the bias of the NN $\\Phi^\\wedge$ with ReLU activation, seen during the lecutres that emulates $F_1$ and plot in on [0,1] \n",
    "$$\n",
    "F_1 : x\\mapsto \n",
    "\\begin{cases}\n",
    "2x &x\\in (0, 1/2]\\\\\n",
    "2-2x &x \\in (1/2, 1)\\\\\n",
    "0 &\\text{elsewhere.}\n",
    "\\end{cases}\n",
    "$$\n",
    "We want to define the hidden weight $W_1 \\in \\mathbb{R}^{3\\times 1}$ and bias $b\\in \\mathbb{R}^{1\\times 3}$ and the output weight $W_2\\in \\mathbb{R}^{1\\times 3}$. The NN then will be the map\n",
    "$$\n",
    "x\\mapsto \\left[\\mathrm{ReLU}(xW_1^\\top + b)\\right] W_2^\\top\n",
    "$$\n",
    "where $x \\in \\mathbb{R}^{N\\times 1}$ can be a vector of $N$ points in $[0,1]$. Note that we choose the formulation above to follow the format implemented in `torch.nn.linear`, see https://pytorch.org/docs/stable/generated/torch.nn.Linear.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_weight_mat = torch.tensor( ) # this should be W_1 \n",
    "input_bias_vec = # b\n",
    "output_weight_mat = # this should be W_2\n",
    "\n",
    "with torch.inference_mode(): #to disable gradient computations, which conflicts with plotting\n",
    "    x = torch.linspace(0, 1, 200).unsqueeze(-1)\n",
    "    y = # implement the function above. you can use torch.relu(...). Matrix-matrix product is done with the @ operator\n",
    "\n",
    "    x_np = x.cpu()\n",
    "    \n",
    "    plt.plot(x_np, y.squeeze().cpu(), label='f')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do the same thing using `torch.nn.Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = torch.nn.Linear(, ) # complete with the right dimensions\n",
    "output_layer = torch.nn.Linear(, , bias=False) # complete with the right dimensions\n",
    "\n",
    "hidden_layer.weight = torch.nn.Parameter( ) #complete with the right mat\n",
    "hidden_layer.bias = torch.nn.Parameter( ) #complete with the right mat\n",
    "output_layer.weight = torch.nn.Parameter( ) #complete with the right mat\n",
    "\n",
    "with torch.inference_mode():\n",
    "    x = torch.linspace(0, 1, 200).unsqueeze(-1)\n",
    "    y = # use hidden_layer, output_layer, and torch.relu here. Do not use hidden_layer.weight, hidden_layer.bias, etc.\n",
    "\n",
    "    x_np = x.cpu()\n",
    "    \n",
    "    plt.plot(x_np, y.squeeze().cpu(), label='f')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second step: define a general Feed-Forward Neural Network\n",
    "\n",
    "Complete the following Python class of a FNN (This will be used in future laboratories!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNet(torch.nn.Module):\n",
    "    def __init__(self, activation, n_input, n_hidden=None, n_output=None):\n",
    "        \"\"\"\n",
    "        construct a NN with\n",
    "        activation: activation function\n",
    "        n_input: input dimension\n",
    "        n_hidden: list of hidden layer widths\n",
    "        n_output: output dim\n",
    "        example:  NN = DeepNet(torch.tanh, 2, [4, 3, 4], 1)\n",
    "        \"\"\"\n",
    "        super(DeepNet, self).__init__()  # Constructor of the super class torch.nn.Module\n",
    "        torch.manual_seed(0) # set the seed for reproducibility\n",
    "        self.dim_in = # input dimension (given as arg in constructor)\n",
    "        self.activation = # activation function (given as arg in constructor)\n",
    "        self.hidden = torch.nn.ModuleList() #initialize the inner layers\n",
    "        if n_hidden is not None:\n",
    "            self.L = # number of hidden layers\n",
    "            self.widths = # the list given as argument in the constructor\n",
    "            self.hidden.append( ) # the input layer, as a torch.nn.Linear\n",
    "            torch.nn.init.xavier_normal_(self.hidden[0].weight)\n",
    "            torch.nn.init.normal_(self.hidden[0].bias)\n",
    "            for i in range(1, self.L):\n",
    "                self.hidden.append( ) # the hidden layers, as torch.nn.Linear\n",
    "                torch.nn.init.xavier_normal_(self.hidden[i].weight)\n",
    "                torch.nn.init.normal_(self.hidden[i].bias)\n",
    "        else:\n",
    "            self.L = 0\n",
    "\n",
    "        if n_output is not None:\n",
    "            self.dim_out = # output dimension\n",
    "            self.output = torch.nn.Linear( ) # the output layer as a torch.nn.Linear - no bias here\n",
    "            torch.nn.init.xavier_normal_(self.output.weight)\n",
    "        else:\n",
    "            self.output = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Given input vector x produces the output of the NN\n",
    "        \"\"\"\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(-1) # add a dimension at the end\n",
    "        for i in range(self.L):\n",
    "            x = # the affine transformation (we have defined this layer in the constructor)\n",
    "            x = # the nonlinearity (we have given the activation in the constructor)\n",
    "            \n",
    "        if self.output is not None:\n",
    "            x = # the output layer\n",
    "        return x\n",
    "\n",
    "    #Functions needed for Exercise 2\n",
    "    def set_weight(self, layer, weight_mat, requires_grad=True):\n",
    "        assert self.L > layer\n",
    "        assert weight_mat.shape == self.hidden[layer].weight.shape\n",
    "        # set the weight matrix for the layer with index \"layer\". We have to use torch.nn.Parameter, as we have already done\n",
    "\n",
    "    def set_bias(self, layer, bias_mat):\n",
    "        assert self.L > layer\n",
    "        assert bias_mat.flatten().shape == self.hidden[layer].bias.shape\n",
    "        # the bias of layer with index \"layer\"\n",
    "\n",
    "    def set_output_weight(self, weight_mat, requires_grad=True):\n",
    "        assert self.output is not None\n",
    "        assert weight_mat.shape == self.output.weight.shape\n",
    "        # set the output weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a FNN with 1 layer 3 neurons and ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_d = # input dim\n",
    "output_d = # output dim\n",
    "hidden_layer = # the list of widths of (one) hidden layer\n",
    "activation_function = # relu activation\n",
    "\n",
    "FNN = # use the class created above\n",
    "FNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefine $\\Phi^\\wedge$ using the FNN defined before  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phiwedge = FNN \n",
    "\n",
    "# set the hidden weight/bias and the output weight of Phiwedge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the output of the NN in [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    x = torch.linspace(0, 1, 200)\n",
    "    y = Phiwedge(x)\n",
    "\n",
    "    x_np = x.cpu()\n",
    "    \n",
    "    plt.plot(x_np, y.cpu(), label='f')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third step: Concatenation of two Networks\n",
    "Complete the following code which implements the non-sparse concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(nn1, nn2):\n",
    "    assert nn1.dim_out == nn2.dim_in\n",
    "    assert nn1.activation == nn2.activation\n",
    "    new_L = # numer of hidden layers of concatenated network\n",
    "    new_widths = # concatenate the two lists (+ operator in python)\n",
    "    out_net = # construct the network\n",
    "\n",
    "    for i in range(nn1.L):\n",
    "        out_net.set_weight( ) \n",
    "        out_net.set_bias( )\n",
    "    new_weight = # compute new weight, which is the product of the first weight of nn2 and the output weight of nn1\n",
    "    out_net.set_weight( )\n",
    "    out_net.set_bias( )\n",
    "    for i in range(1, nn2.L):\n",
    "        out_net.set_weight( )\n",
    "        out_net.set_bias( )\n",
    "    out_net.set_output_weight( )\n",
    "\n",
    "    return out_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the NN $\\Phi^\\wedge$ defined in the second step 4 time ($F_4 = F_1\\circ F_1 \\circ F_1 \\circ F_1$) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F4 = # concatenate Phiwedge 4 times\n",
    "print(F4)\n",
    "with torch.inference_mode():\n",
    "    x = torch.linspace(0, 1, 400)\n",
    "    x_np = x.cpu().numpy()\n",
    "\n",
    "    # plot F4(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth step: Define the Yarotsky Network \n",
    "\n",
    "We now want to construct the network from Yarotsky's paper. Defining by $F_n$ the $n$-fold composition of $\\Phi^\\wedge$, we use the fact that\n",
    "$$\n",
    "x - \\sum_{k=1}^N \\frac{F_k(x)}{4^k} \\to x^2, \\qquad \\text{as }N\\to\\infty,\n",
    "$$ \n",
    "for all $x\\in[0,1]$. The network therefore uses a non-standard architecture, as shown in this picture:\n",
    "\n",
    "<center>\n",
    "    <img src=\"./Yarotsky-arch.png\" alt=\"Architecture\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YarotskyNet(DeepNet):\n",
    "    def __init__(self, net, out_weights):\n",
    "        \"\"\"\n",
    "        Initialize by copying a net given as input, and a vector of\n",
    "        weights that are the weights of the skip connections\n",
    "        \"\"\"\n",
    "        super(YarotskyNet, self).__init__(net.activation, net.dim_in, net.widths, net.dim_out)\n",
    "        self.out_weights = # these are the weights of the skip connections, and are passed as an argument to the constructor\n",
    "        for i in range(net.L):\n",
    "            self.hidden[i] = # copy the hidden weights of the NN given as argument (net)\n",
    "        \n",
    "        self.output = # copy also the output weight\n",
    "        \n",
    "        assert len(out_weights) == len(net.widths) + 1\n",
    " \n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x[:, None]\n",
    "        \n",
    "        y = # the first skip connection goes straight from the input\n",
    "        for i in range(self.L):\n",
    "            x = # affine transformation\n",
    "            x = # activation\n",
    "            y = # skip connection to the output (use self.output_weights[i+1] here)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Yarotsky network, with depth = 3. Plot the output of the Yarotsky network over [0,1] and compare it with respect to $x^2$. Then evaluate the $L^2$ error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    depth = 3\n",
    "\n",
    "    out_weights = # define the weights of the skip connections to the output for Yarotsy's network\n",
    "   \n",
    "\n",
    "    basenet = Phiwedge\n",
    "    for j in range(depth-2):\n",
    "        # concatenate the Phiwedge network, to obtain a network with the right weights but no skip connections\n",
    "        \n",
    "\n",
    "    squarenet = # construct the network with the class we have defined, copying from the one we have concatenated, but also putting in skip connections\n",
    "\n",
    "    x = torch.linspace(0, 1, 100)\n",
    "    x_np = x.cpu().numpy()\n",
    "    \n",
    "    y = # complete\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    plt.plot(x_np, y.cpu().numpy(), label='Yarotsky')\n",
    "    plt.plot(x_np, x_np**2, 'r-.',label = '$x^2$')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'L2 error: {np.linalg.norm(y.cpu().numpy().flatten()-x_np**2)/10:4e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalute the $L^2$ error of the Yarotsky Network with respect to $x^2$, as a function of the depth of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_err(depth):\n",
    "\n",
    "    # define squarenet as above \n",
    "    \n",
    "    Npts = 10**6\n",
    "    x = torch.linspace(0, 1, Npts)\n",
    "    y = squarenet(x)\n",
    "    x_np = x.cpu().detach().numpy()\n",
    "\n",
    "    return np.linalg.norm(y.cpu().detach().numpy().flatten()-x_np**2)/np.sqrt(Npts)\n",
    "\n",
    "err = []\n",
    "all_depths = range(3, 15)\n",
    "for depth in all_depths:\n",
    "    err.append(compute_err(depth))\n",
    "\n",
    "plt.semilogy(all_depths, err, 'o-')\n",
    "plt.grid()\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.ylabel(\"$L^2$ error\")\n",
    "plt.show()\n",
    "\n",
    "err"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
