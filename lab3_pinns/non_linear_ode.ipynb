{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import torch.nn as nn\n",
    "from DNN import DeepNet\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device) # default tensor device\n",
    "print(\"I'm using: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A non-linear boundary value problem\n",
    "We consider the problem\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\displaystyle -\\frac{d}{dx}\\Big[(1+u(x)^2)\\frac{d}{dx}u\\Big] = -2\\pi^2\\cos(\\pi x)^2\\sin(\\pi x) +  \\pi^2 \\sin(\\pi x)\\big(\\sin(\\pi x)^2 +1\\big) =: f(x), \\quad x \\in (0,2) \\\\[15pt] \n",
    "    u(0) = u(2) = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "Which is solved by $u(x) = \\sin(\\pi x)$.\n",
    "\n",
    "We start by introducing some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "  return x.cpu().detach().numpy()\n",
    "\n",
    "def L2_diff(y, y_pred):\n",
    "  return np.linalg.norm(y-y_pred,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a network `FNN`  with 5 layers, 20 neurons and tanh (https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#tanh) as the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `torch.rand`, define the points that will be used for training. For the test points, we use equispaced points that can be defined with `torch.linspace`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.Tensor([0]).to(device)\n",
    "x1 = torch.Tensor([2]).to(device)\n",
    "u_b0 = torch.Tensor([0]).to(device)\n",
    "u_b1 = torch.Tensor([0]).to(device)\n",
    "\n",
    "n_train_points = \n",
    "n_test_points  = \n",
    "\n",
    "x_train = \n",
    "x_sort_train,_ = torch.sort(x_train,0)\n",
    "x_test  = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(8, 1))  \n",
    "plt.scatter(convert(x_train), np.zeros_like(convert(x_train)), color='red', s=50, marker='x', label='Collocation Points')\n",
    "plt.scatter(convert(x0), np.zeros_like(convert(x0)), color='green', s=50, label='Collocation Points')\n",
    "plt.scatter(convert(x1), np.zeros_like(convert(x1)), color='green', s=50, label='Collocation Points')\n",
    "plt.xlabel('x')\n",
    "plt.yticks([])  \n",
    "plt.title('Collocation Points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the exact solution and right hand side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhs(x):\n",
    "    # input a torch tensor, output a torch tensor\n",
    "\n",
    "def exact_sol(x):\n",
    "    # input a torch tensor, output a numpy one, using the function convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we define a function that computes the loss from \n",
    "- the NN $\\Phi$ approximating the solution\n",
    "- a vector of points $(x_i)_i$\n",
    "- the right hand side function $f$\n",
    "- a metric computing the distance between the lhs and the rhs\n",
    "The function has to compute the term\n",
    "$$\n",
    "-\\frac{d}{dx}\\left((1+\\Phi^2)\\frac{d}{dx}\\Phi\\right)\n",
    "$$\n",
    "on the vector of points $(x_i)_i$.  To compute first and second derivatives use `torch.autograd.grad`. You will have to `create_graph=True` and `retain_graph=True`.\n",
    "\n",
    "The second function exaluates the error at a point: it should return $(\\Phi(x) - y)^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss_ode(Phi, x, rhs, metric):\n",
    "    x.requires_grad_(True) \n",
    "    ## complete the function\n",
    "\n",
    "def eval_loss_pt(Phi, x, y):\n",
    "    ## complete the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the network, as in the previous case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam( ) ## initialize with the right arguments\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, 0.96)\n",
    "\n",
    "loss_values = np.empty([n_epochs+1,4]); #total,ode, b0,b1\n",
    "MSE =  ## the same as in the previous notebook\n",
    "STEP = 100 #save every 100 epochs\n",
    "\n",
    "assert n_epochs%STEP == 0\n",
    "\n",
    "err = np.empty([int(n_epochs/STEP)+1,2])\n",
    "u_animate =  np.empty([int(n_epochs/STEP)+1,n_train_points])\n",
    "err_animate = np.empty([int(n_epochs/STEP)+1,n_train_points])\n",
    "k = 0 \n",
    "\n",
    "rhs_train = rhs(x_train)\n",
    "u_exact_train = exact_sol(x_train)\n",
    "rhs_train_sort = rhs(x_sort_train)\n",
    "u_exact_train_sort = exact_sol(x_sort_train)\n",
    "\n",
    "rhs_test = rhs(x_test)\n",
    "u_exact_test = exact_sol(x_test)\n",
    "\n",
    "for epoch in range(n_epochs+1):\n",
    "    \n",
    "    ## compute here the three components of the loss: the residual and the two boundary conditions, \n",
    "    ## then sum them up\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    ## differentiate the loss as in the previous notebook (one line)\n",
    "\n",
    "    ## one step of optimization (one line)\n",
    "    \n",
    "\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    loss_values[epoch,:] = np.stack([convert(loss),convert(loss_ode),convert(loss_b0),convert(loss_b1)])\n",
    "\n",
    "    if epoch%STEP == 0:\n",
    "      lr = np.array(scheduler.get_last_lr())\n",
    "      u_NN_test =  FNN(x_test)\n",
    "      err[k,:] = [L2_diff(convert(FNN(x_train)),u_exact_train), L2_diff(convert(u_NN_test),u_exact_test)]\n",
    "      u_animate[k,:]  = convert(FNN(x_sort_train).squeeze())\n",
    "      err_animate[k,:]= (np.abs(u_animate[k,:] - u_exact_train_sort.squeeze()))\n",
    "      print(f'Epoch {epoch} || learning rate {lr.squeeze():.2e}  || Error  Train {err[k,0]:.2e} || Error Test  {err[k,1]:.2e}''\\n'\n",
    "            f'Global loss {loss_values[epoch,0]:.2e} || loss ode {loss_values[epoch,1]:.2e} || loss b0 {loss_values[epoch,2]:.2e} || loss b1 {loss_values[epoch,3]:.2e}','\\n')\n",
    "      k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the results\n",
    "Value of the loss function at the vary of the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(n_epochs+1)\n",
    "#y_lims = [np.min(loss_values)*0.8,np.max(loss_values)*1.2]\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize = (15,7), num =4)\n",
    "axs[0].semilogy(epochs,loss_values[:,0])\n",
    "axs[0].set_xlabel('Numeber of Epochs')\n",
    "#axs[0].set_ylim(y_lims)\n",
    "axs[0].grid('on')\n",
    "axs[0].set_title('Global loss')\n",
    "\n",
    "axs[1].semilogy(epochs,loss_values[:,1],label = 'ODE')\n",
    "axs[1].semilogy(epochs,loss_values[:,2],label = 'b0' )\n",
    "axs[1].semilogy(epochs,loss_values[:,3],label = 'b1' )\n",
    "axs[1].set_xlabel('Numeber of Epochs')\n",
    "#axs[1].set_ylim(y_lims)\n",
    "axs[1].legend()\n",
    "axs[1].grid('on')\n",
    "axs[1].set_title('Components loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the exact solution with the PINN prediction and evaluate the point-wise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = convert(x_test)\n",
    "u_NN_test = convert(u_NN_test)\n",
    "x_sort_train = convert(x_sort_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize = (15,7))\n",
    "axs[0].plot(x_test,u_exact_test,label = 'Exact')\n",
    "axs[0].plot(x_test,u_NN_test, label = 'Test prediction')\n",
    "axs[0].set_xlabel('x')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('Comparision')\n",
    "\n",
    "axs[1].semilogy(x_test,np.abs(u_exact_test-u_NN_test))\n",
    "axs[1].set_title('Point-wise error')\n",
    "axs[1].set_xlabel('x')\n",
    "axs[1].set_ylabel('|u - $u_{NN}$|')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L^2$ Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(n_epochs+1)\n",
    "y_lims = [np.min(loss_values)*0.8,np.max(loss_values)*1.2]\n",
    "\n",
    "plt.semilogy(epochs,loss_values[:,0],label = 'values of the loss')\n",
    "plt.semilogy(epochs[0:-1:STEP-1],err[:,0],'-x', label = '$L^2$ Training error')\n",
    "plt.semilogy(epochs[0:-1:STEP-1],err[:,1],'-x', label = '$L^2$ Test error')\n",
    "\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.grid('on')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animation of the solution and of the point wise error during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "fig, axs = plt.subplots(1,2, figsize = (15,7),num = 3)\n",
    "\n",
    "def updata_data(frame):\n",
    "    axs[0].clear()\n",
    "    axs[0].plot(x_sort_train,u_animate[frame,:],'b--',label = r'$y_{\\Theta}$')\n",
    "    axs[0].plot(x_sort_train,u_exact_train_sort,'r', label = 'Exact')\n",
    "    axs[0].set_ylim([-np.max(u_animate),np.max(u_animate)])\n",
    "    axs[0].set_xlim([0,2])\n",
    "    axs[0].set_xlabel('t')\n",
    "\n",
    "    axs[0].grid('on')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].clear()\n",
    "    axs[1].semilogy(x_sort_train,err_animate[frame,:])\n",
    "    axs[1].grid('on')\n",
    "    axs[1].set_ylim([np.min(err_animate)*0.9,np.max(err_animate)*1.1])\n",
    "    axs[1].set_xlabel('t')\n",
    "    axs[1].set_title(r'$|y - y_{\\Theta}|$')\n",
    "    plt.suptitle(f'Epoch {frame*STEP} || error {err[frame,0]:.2e}')\n",
    "    return axs[0]\n",
    "\n",
    "animation = FuncAnimation(fig,func = updata_data,frames=len(u_animate),interval = 500,)\n",
    "plt.show()\n",
    "# for google colab\n",
    "# from IPython.display import HTML\n",
    "# video = HTML(animation.to_html5_video())\n",
    "# display(video)\n",
    "# plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
