{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import torch.nn as nn\n",
    "from DNN import DeepNet\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device) # default tensor device\n",
    "print(\"I'm using: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "  return x.cpu().detach().numpy()\n",
    "\n",
    "def L2_diff(y,y_pred):\n",
    "  return np.linalg.norm(y-y_pred,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple ODE\n",
    "We consider the problem\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\displaystyle \\frac{dy(t)}{dt} =  - 2y(t) =: f(t, y(t)) ,\\ \\ t\\in (0,1)\\\\[15pt]\n",
    "    y(0) = 1 =: y_0 .\n",
    "\\end{cases}\n",
    "$$\n",
    "That we want to approximate with a PINN. This means that we want to replace the solution $y(t)$ by a neural network $\\Phi$, then optimize the parameters of the network to minimize the loss function\n",
    "$$\n",
    "\\mathcal{L}(\\Phi) := \\sum_{i=1}^N \\mathrm{dist}(\\Phi'(t_i), f(t_i, \\Phi(t_i))) + (\\Phi(0)-y_0)^2,\n",
    "$$\n",
    "where $t_1, \\dots, t_N$ are points in $(0,1)$ and for some distance $\\mathrm{dist}$ that we will choose later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a network `FNN`$:\\mathbb{R}\\to\\mathbb{R}$ with 3 layers, 5 neurons and `tanh` (see https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#tanh) as the activation function. Use the `DeepNet` class from the first lab, provided in the `DNN` module that we have imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `torch.rand`, define the points that will be used for training (i.e., where the residuals will be computed). For the points used for testing, use `torch.linspace`. We use $50$ points for training, and test on $100$ points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = torch.Tensor([0]).to(device)\n",
    "y0 = torch.Tensor([1]).to(device)  \n",
    "\n",
    "n_train_points = \n",
    "n_test_points  = \n",
    "\n",
    "t_train =  # we want a (n_train_points x 1) vector here\n",
    "t_sort_train, _ = torch.sort(t_train,0) \n",
    "t_test =  # dimensions: n_test_points x 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 1),num =1)  \n",
    "plt.scatter(convert(t_train), np.zeros_like(convert(t_train)), color='red', s=50, marker='x', label='Collocation Points')\n",
    "plt.scatter(convert(t0), np.zeros_like(convert(t0)), color='green', s=50, label='Collocation Points')\n",
    "plt.xlabel('x')\n",
    "plt.yticks([])  \n",
    "plt.title('Collocation Points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the exact solution and the right hand side function $f(t, y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_sol(t):\n",
    "    ## takes a torch vector as input, returs a numpy vector (use the `convert` function)\n",
    "\n",
    "def rhs(t, y):\n",
    "    ## input: torch, output: torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the loss function for the training, where the metric will be defined later. Some examples can be https://pytorch.org/docs/stable/nn.html#loss-functions.\n",
    "\n",
    "Let $\\Phi$ be the neural network approximating $y$. The first function takes $\\Phi$, a vector of points $\\mathbf{t} = (t_i)_i$, the right hand side function $f:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$, and finally a function `metric`$:\\mathbb{R}^N\\times \\mathbb{R}^N\\to \\mathbb{R}_+$ which computes some distance between the vectors.\n",
    "\n",
    "The function then computes $\\Phi(\\mathbf{t})$ and $\\Phi'(\\mathbf{t})$ and finally it returns \n",
    "$$\n",
    "\\mathtt{metric}(\\Phi'(\\mathbf{t}), f(\\mathbf{t}, \\Phi(\\mathbf{t})))\n",
    "$$\n",
    "\n",
    "The second function computes the (squared) error in the inital condition, i.e., $(\\Phi(0) - y_0)^2$: it takes as arguments $\\Phi$, the initial point $t_0$ and the exact value $y_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss_ode(NN, t, f, metric):\n",
    "\n",
    "\n",
    "def eval_loss_IC(NN, t_0, y_0):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the network: see https://pytorch.org/docs/stable/optim.html for an overview. We start by defining the total number of iteration steps, and initialize the Adam optimizer and learning rate scheduler. Finally, we choose the `metric` function to compute the residuals. We want to use the mean squared error, see https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam( ) ## put the right arguments in\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, 0.98)\n",
    "loss_values = np.empty([n_epochs+1, 3]); #total, ode, 0\n",
    "\n",
    "## this is the `metric` function that we will use\n",
    "MSE =  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set the variables for saving the results and initialize the values of the exact solution on the training and test point sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP = 200  # save every STEP epochs\n",
    "assert n_epochs%STEP == 0\n",
    "\n",
    "err = np.empty([int(n_epochs/STEP)+1, 2])\n",
    "y_animate =  np.empty([int(n_epochs/STEP)+1, n_train_points])\n",
    "err_animate = np.empty([int(n_epochs/STEP)+1, n_train_points])\n",
    "k = 0 \n",
    "\n",
    "y_exact_train = exact_sol(t_train)\n",
    "y_exact_test  = exact_sol(t_test)\n",
    "y_exact_train_sort = exact_sol(t_sort_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now start iterating on the optimization procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs+1):\n",
    "    \n",
    "    loss_ode = eval_loss_ode( ) ## complete this line\n",
    "    \n",
    "    loss_IC = eval_loss_IC( ) ## complete this line\n",
    "\n",
    "    loss = loss_ode + loss_IC\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    ## differentiate the loss function (see: https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html )\n",
    "    ## (one line)\n",
    "\n",
    "\n",
    "    ## make one optimization step (one line, see https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html)\n",
    "\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    loss_values[epoch,:] = np.stack([convert(loss),convert(loss_ode),convert(loss_IC)])\n",
    "\n",
    "    if epoch%STEP == 0:\n",
    "        lr = np.array(scheduler.get_last_lr())\n",
    "        y_NN_test =  FNN(t_test)\n",
    "        err[k,:]  = [L2_diff(convert(FNN(t_train)),y_exact_train), L2_diff(convert(y_NN_test), y_exact_test)]\n",
    "        y_animate[k,:]  = convert(FNN(t_sort_train).squeeze())\n",
    "        err_animate[k,:]= (np.abs(y_animate[k,:] - y_exact_train_sort.squeeze()))\n",
    "        print(f'Epoch {epoch} || learning rate {lr.squeeze():.2e}  || Error  Train {err[k,0]:.2e} || Error Test  {err[k,1]:.2e}''\\n'\n",
    "              f'Global loss {loss_values[epoch,0]:.2e} || loss ode {loss_values[epoch,1]:.2e} || loss IC {loss_values[epoch,2]:.2e}','\\n')\n",
    "        k+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the results\n",
    "Value of the loss function throughout the optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(n_epochs+1)\n",
    "#y_lims = [np.min(loss_values)*0.8,np.max(loss_values)*1.2]\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize = (15,7), num =4)\n",
    "axs[0].semilogy(epochs,loss_values[:,0])\n",
    "axs[0].set_xlabel('Numeber of Epochs')\n",
    "#axs[0].set_ylim(y_lims)\n",
    "axs[0].grid('on')\n",
    "axs[0].set_title('Global loss')\n",
    "\n",
    "axs[1].semilogy(epochs,loss_values[:,1],label = 'ODE')\n",
    "axs[1].semilogy(epochs,loss_values[:,2],label = 'IC' )\n",
    "axs[1].set_xlabel('Numeber of Epochs')\n",
    "#axs[1].set_ylim(y_lims)\n",
    "axs[1].legend()\n",
    "axs[1].grid('on')\n",
    "axs[1].set_title('Components loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the exact solution with the PINN prediction and evaluate the pointwise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test = convert(t_test)\n",
    "y_NN_test = convert(y_NN_test)\n",
    "t_sort_train = convert(t_sort_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize = (15,7))\n",
    "axs[0].plot(t_test,y_exact_test,label = 'Exact')\n",
    "axs[0].plot(t_test,y_NN_test, label = 'Test prediction')\n",
    "axs[0].set_xlabel('t')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('Comparision')\n",
    "\n",
    "axs[1].semilogy(t_test,np.abs(y_exact_test-y_NN_test))\n",
    "axs[1].set_title('Point-wise error')\n",
    "axs[1].set_xlabel('t')\n",
    "axs[1].set_ylabel('|y - $y_{NN}$|')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L^2$ Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = range(n_epochs+1)\n",
    "y_lims = [np.min(loss_values)*0.8,np.max(loss_values)*1.2]\n",
    "plt.figure(4)\n",
    "plt.semilogy(epochs,loss_values[:,0],label = 'values of the loss')\n",
    "plt.semilogy(epochs[0:-1:STEP-1],err[:,0],'-x', label = '$L^2$ Training error')\n",
    "plt.semilogy(epochs[0:-1:STEP-1],err[:,1],'-x', label = '$L^2$ Test error')\n",
    "\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.grid('on')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animation of the solution and of the pointwise error during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "fig, axs = plt.subplots(1,2, figsize = (15,7),num = 3)\n",
    "\n",
    "def updata_data(frame):\n",
    "    axs[0].clear()\n",
    "    axs[0].plot(t_sort_train,y_animate[frame,:],'b--',label = r'$y_{\\Theta}$')\n",
    "    axs[0].plot(t_sort_train,y_exact_train_sort,'r', label = 'Exact')\n",
    "    axs[0].set_ylim([0,1.1])\n",
    "    axs[0].set_xlim([0,1])\n",
    "    axs[0].set_xlabel('t')\n",
    "\n",
    "    axs[0].grid('on')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].clear()\n",
    "    axs[1].semilogy(t_sort_train,err_animate[frame,:])\n",
    "    axs[1].grid('on')\n",
    "    axs[1].set_ylim([np.min(err_animate)*0.9,np.max(err_animate)*1.1])\n",
    "    axs[1].set_xlabel('t')\n",
    "    axs[1].set_title(r'$|y - y_{\\Theta}|$')\n",
    "    plt.suptitle(f'Epoch {frame*STEP} || error {err[frame,0]:.2e}')\n",
    "    return axs[0]\n",
    "\n",
    "animation = FuncAnimation(fig,func = updata_data,frames=len(y_animate),interval = 500,)\n",
    "plt.show()\n",
    "#for google colab\n",
    "#from IPython.display import HTML\n",
    "#video = HTML(anim.to_html5_video())\n",
    "#display(video)\n",
    "#plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
