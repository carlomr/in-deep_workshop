{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving an Inverse Problem with Physics-Informed Neural Networks (PINNs)\n",
    "\n",
    "### Problem Statement:\n",
    "We aim to find the scalar parameter $\\alpha \\in \\mathbb{R}$ given a target function $\\tilde{u}$. Specifically, we want to solve the following optimization problem:\n",
    "$$\n",
    "\\text{arg}\\min f(u,\\tilde{u}) = \\| u - \\tilde{u} \\|^2,\n",
    "$$\n",
    "where $u$ satisfies the boundary value problem described by a second-order Partial differential equation (PDE):\n",
    "\\begin{cases}\n",
    "\\displaystyle -\\alpha \\frac{d^2u}{dx^2} = f(x), \\quad x \\in \\Omega = (0,1),\\\\​\n",
    " u(0) = u(1) =  0 . \n",
    "\\end{cases}\n",
    "\n",
    "### Given data:\n",
    "The target function $\\tilde{u}$ is given by:\n",
    "$$\\tilde{u} = \\sin(\\pi x)$$\n",
    "  \n",
    "The function $f(x)$, derived from the assumed true solution, is specified as:\n",
    "$$f = 2 \\pi^2 \\sin(\\pi x).$$\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "# Objective\n",
    "\n",
    "Our goal is to compute the value of $\\alpha$ that best aligns the solution $u(x)$ of the PDE with the given $\\tilde{u}(x)$.\n",
    "To achieve this, we can utilize a Physics-Informed Neural Network (PINN) that integrates the differential equation as a part of its loss function.\n",
    "This approach ensures that the learned solution not fits the data while respecting the underlying \"physics\" described by the PDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from DNN import DeepNet\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.manual_seed(128)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "  return x.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input =1 \n",
    "output =1\n",
    "hidden_layer = 4*[50]\n",
    "activation_function = nn.Tanh()\n",
    "\n",
    "FNN = DeepNet(activation_function,input,hidden_layer,output)\n",
    "alpha = nn.Parameter(torch.Tensor([1]).requires_grad_(True)).to(device)\n",
    "FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.Tensor([0]).to(device)\n",
    "x1 = torch.Tensor([1]).to(device)\n",
    "u_b0 = torch.Tensor([0]).to(device)\n",
    "u_b1 = torch.Tensor([0]).to(device)\n",
    "\n",
    "n_train_points = 150\n",
    "n_data_points  = 60\n",
    "n_test_points  = 100\n",
    "\n",
    "x_train = torch.rand([n_train_points,1])\n",
    "x_data = torch.linspace(0,1,n_data_points).unsqueeze(1)\n",
    "x_test = torch.linspace(0,1,n_test_points).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 1),num =1)  \n",
    "plt.scatter(convert(x_train), np.zeros_like(convert(x_train)), color='red', s=50, marker='x', label='Collocation Points')\n",
    "plt.scatter(convert(x_data), np.zeros_like(convert(x_data)), color='blue', s=50, marker='.', label='Data Points')\n",
    "\n",
    "plt.scatter(convert(x0), np.zeros_like(convert(x0)), color='green', s=50)\n",
    "plt.scatter(convert(x1), np.zeros_like(convert(x1)), color='green', s=50)\n",
    "plt.xlabel('x')\n",
    "plt.yticks([])  \n",
    "plt.title('Collocation Points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_sol(x):\n",
    "    return torch.sin(torch.pi*x)\n",
    "\n",
    "alpha_exact = 2\n",
    "\n",
    "def rhs(x):\n",
    "    return alpha_exact*torch.pi**2*torch.sin(torch.pi*x)\n",
    "\n",
    "u_exact_train = exact_sol(x_train)\n",
    "rhs_train = rhs(x_train)\n",
    "u_exact_data = exact_sol(x_data)\n",
    "u_exact_test = exact_sol(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss_ode(Phi,x,rhs,alpha,metric):\n",
    "    x.requires_grad_(True)\n",
    "\n",
    "    u_NN = Phi(x)\n",
    "    u_NN_x = torch.autograd.grad(u_NN.sum(),x,create_graph=True,retain_graph=True,allow_unused=True)[0]\n",
    "    u_NN_xx = torch.autograd.grad(u_NN_x.sum(),x,create_graph=True)[0]\n",
    "\n",
    "    return metric(-alpha*u_NN_xx, rhs(x))\n",
    "\n",
    "def eval_loss_pt(Phi,x,y):\n",
    "    return (Phi(x) -y ).pow(2).squeeze()\n",
    "\n",
    "def eval_loss_data(Phi,x,data,metric):\n",
    "    return metric(Phi(x),data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "learining_rate = 1e-3\n",
    "alpha = nn.Parameter(torch.Tensor([1]).to(device).requires_grad_(True)) #! l'ordine tra device e requires_grad e' fondamentale altrimenti non funziona perche' non e' un leaf\n",
    "\n",
    "optimizer = optim.Adam(list(FNN.parameters())+[alpha],lr = 1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,5000,0.5)\n",
    "\n",
    "loss_values = np.empty([n_epochs+1,5]) #total,ode, b0,b1, data\n",
    "alpha_values = np.empty([n_epochs+1])\n",
    "\n",
    "MSE =  torch.nn.MSELoss()\n",
    "STEP = 100 #save every 100 epochs\n",
    "\n",
    "assert n_epochs%STEP == 0\n",
    "\n",
    "err = np.empty([int(n_epochs/STEP)+1,2])\n",
    "u_animate =  np.empty([int(n_epochs/STEP)+1,n_train_points])\n",
    "err_animate = np.empty([int(n_epochs/STEP)+1,n_train_points])\n",
    "k = 0 \n",
    "\n",
    "for epoch in range(n_epochs+1):\n",
    "\n",
    "    loss_ode = eval_loss_ode(FNN, x_train, rhs,alpha, MSE)\n",
    "\n",
    "    loss_b0 = eval_loss_pt(FNN, x0, u_b0)\n",
    "    loss_b1 = eval_loss_pt(FNN, x1, u_b1)\n",
    "\n",
    "    loss_data = eval_loss_data(FNN,x_data,u_exact_data,MSE)\n",
    "\n",
    "    loss = loss_ode +  loss_b0 + loss_b1  + 1.5*loss_data\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    loss_values[epoch,:] = np.stack([convert(loss),convert(loss_ode),convert(loss_b0),convert(loss_b1),convert(loss_data)])\n",
    "    alpha_values[epoch] = alpha\n",
    "    if epoch%STEP == 0:\n",
    "      lr = np.array(scheduler.get_last_lr())\n",
    "    #   u_NN_test =  FNN(x_test)\n",
    "    #   err[k,:] = [L2_diff(convert(FNN(x_train)),u_exact_train), L2_diff(convert(u_NN_test),u_exact_test)]\n",
    "    #   u_animate[k,:]  = convert(FNN(x_sort_train).squeeze())\n",
    "    #   err_animate[k,:]= (np.abs(u_animate[k,:] - u_exact_train_sort.squeeze()))\n",
    "      print(f'Epoch {epoch} || learning rate {lr.squeeze():.2e} ''\\n'\n",
    "            f'Global loss {loss_values[epoch,0]:.2e} || loss ode {loss_values[epoch,1]:.2e} || loss b0 {loss_values[epoch,2]:.2e} || loss b1 {loss_values[epoch,3]:.2e} || loss data {loss_values[epoch,4]:.2e}','\\n')\n",
    "      k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(n_epochs+1)\n",
    "#y_lims = [np.min(loss_values)*0.8,np.max(loss_values)*1.2]\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize = (15,7), num =4)\n",
    "axs[0].semilogy(epochs,loss_values[:,0])\n",
    "axs[0].set_xlabel('Numeber of Epochs')\n",
    "#axs[0].set_ylim(y_lims)\n",
    "axs[0].grid('on')\n",
    "axs[0].set_title('Global loss')\n",
    "\n",
    "axs[1].semilogy(epochs,loss_values[:,1],label = 'ODE')\n",
    "axs[1].semilogy(epochs,loss_values[:,2],label = 'b0' )\n",
    "axs[1].semilogy(epochs,loss_values[:,3],label = 'b1' )\n",
    "axs[1].semilogy(epochs,loss_values[:,4],label = 'data' )\n",
    "axs[1].set_xlabel('Numeber of Epochs')\n",
    "#axs[1].set_ylim(y_lims)\n",
    "axs[1].legend()\n",
    "axs[1].grid('on')\n",
    "axs[1].set_title('Components loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_NN_test = convert(FNN(x_test))\n",
    "x_test = convert(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize = (15,7))\n",
    "axs[0].plot(x_test,convert(u_exact_test),label = 'Exact')\n",
    "axs[0].plot(x_test,u_NN_test, label = 'Test prediction')\n",
    "axs[0].set_xlabel('x')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('Comparision')\n",
    "\n",
    "axs[1].semilogy(x_test,np.abs(convert(u_exact_test)-u_NN_test))\n",
    "axs[1].set_title('Point-wise error')\n",
    "axs[1].set_xlabel('x')\n",
    "axs[1].set_ylabel('|u - $u_{NN}$|')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs,alpha_values)\n",
    "plt.plot(epochs,alpha_exact*np.ones(alpha_values.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
